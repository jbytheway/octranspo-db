#!/usr/bin/env python3

# This is a small script intended to convert a zip dump of the OcTranspo
# schedule data (from http://data.ottawa.ca/dataset/oc-transpo-schedules) to a
# sqlite database

import sys
import os
import zipfile
import csv
import io
import datetime
import sqlalchemy
from sqlalchemy.orm import sessionmaker
from sqlalchemy import sql

from orm import *

DATE_FORMAT = '%Y%m%d'

zip_filename, sqlite_filename = sys.argv[1:]

# Set up the DB
if os.path.exists(sqlite_filename):
    os.remove(sqlite_filename)
engine = sqlalchemy.create_engine('sqlite:///' + sqlite_filename, echo=False)
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)
session = Session()

class InsertAs(object):
    def __init__(self, type_):
        self._type = type_

    def __call__(self, row, session):
        session.add(self._type(**row))

def get_service_id(name):
    mapping = ServiceDay.service_mapping
    return mapping.setdefault(name, len(mapping))

ignore_schedules = set()

def set_schedules(row, session):
    service_id = get_service_id(row['service_id'])
    date = row['date']
    type = row['exception_type']

    if type == "2":
        # Schedule is removed from this day; mark it as one to ignore later
        ignore_schedules.add((service_id, date))
    else:
        assert type == "1"
        # Service is added for this date; insert it
        session.add(ServiceDay(date=date, service_id=service_id))

def parse_date(date):
    return datetime.datetime.strptime(date, DATE_FORMAT).date()

def inclusive_date_range(start_date, end_date):
    for n in range((end_date - start_date).days + 1):
        yield start_date + datetime.timedelta(n)

def extend_schedules(row, session):
    service_id = get_service_id(row['service_id'])
    start_date = parse_date(row['start_date'])
    end_date = parse_date(row['end_date'])

    week = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday',
            'saturday', 'sunday']
    weekdays = set()
    for i, day in enumerate(week):
        if row[day] == "1":
            weekdays.add(i)

    for date in inclusive_date_range(start_date, end_date):
        if date.weekday() in weekdays:
            date_string = date.strftime(DATE_FORMAT)
            if (service_id, date_string) not in ignore_schedules:
                session.add(
                        ServiceDay(date=date_string, service_id=service_id))

csv_handlers = [
        ("calendar_dates.txt", set_schedules),
        ("calendar.txt", extend_schedules),
        ("stops.txt", InsertAs(Stop)),
        ("routes.txt", InsertAs(Route)),
        ("trips.txt", InsertAs(Trip)),
        ("stop_times.txt", InsertAs(StopTime)),
        ]

with zipfile.ZipFile(zip_filename) as zip_file:
    for csv_filename, handler in csv_handlers:
        sys.stderr.write("Handling "+csv_filename)
        sys.stderr.flush()
        with zip_file.open(csv_filename) as csv_file:
            csv_stream = io.TextIOWrapper(csv_file, encoding='utf-8', newline='')
            if csv_filename != 'stops.txt':
                # Read the BOM (present in all files but stops, for some
                # reason)
                csv_stream.read(1)
            numRows = 0
            for row in csv.DictReader(csv_stream):
                handler(row, session)
                numRows += 1
                if numRows >= 10000:
                    sys.stderr.write(".")
                    sys.stderr.flush()
                    numRows = 0
                    session.commit()
            sys.stderr.write(".")
            sys.stderr.flush()
            session.commit()
        sys.stderr.write("\n")
        sys.stderr.flush()

# Have completely finished the import now, but we want to add some convenience
# columns on top of what the database contains by default

sys.stderr.write("Finding most popular headsigns...\n")
for route in session.query(Route).all():
    for direction_id in (0, 1):
        # Find the most popular headsign for this route and save it as the
        # modal one
        trips = session.query(
            Trip.trip_headsign, sql.func.count('*').label('trip_count')
            ).filter(Trip.route_id == route.route_id,
                    Trip.direction_id == direction_id
            ).group_by(Trip.trip_headsign).order_by('trip_count')[:1]
        if not trips:
            continue
        trip = trips[0]
        session.add(DirectedRoute(
            route_id=route.route_id, direction_id=direction_id,
            route_modal_headsign=trip.trip_headsign))

session.commit()

sys.stderr.write("Finding maximum sequence numbers...\n")
for trip in session.query(Trip).all():
    # Store for each trip the last stop_sequence number for efficient access to
    # last stop in the trip
    stopTime = session.query(
        sql.func.max(StopTime.stop_sequence).label('max_stop_sequence')
        ).filter(StopTime.trip_id == trip.trip_id
        ).group_by(StopTime.trip_id).one()
    trip.last_stop_sequence = stopTime.max_stop_sequence

session.commit()

sys.stderr.write("Finding routes at each stop...\n")
# Precompute for every stop the routes that go through it
routes_at_stop = {}

for stop_time, trip, route, directed_route, stop in session.query(
        StopTime, Trip, Route, DirectedRoute, Stop
        ).filter(StopTime.trip_id == Trip.trip_id
        ).filter(Trip.route_id == Route.route_id
        ).filter(Route.route_id == DirectedRoute.route_id
        ).filter(Trip.direction_id == DirectedRoute.direction_id
        ).filter(StopTime.stop_id == Stop._id
        ).all():
    routes_at_stop.setdefault(stop, set()).add(directed_route)

sys.stderr.write("Storing routes at each stop...\n")
# Now store all those
for stop, routes in routes_at_stop.items():
    stop.routes.extend(routes)

session.commit()
